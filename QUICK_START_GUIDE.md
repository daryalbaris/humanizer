# AI Humanizer - Quick Start Guide

**Version:** 1.0 | **Target:** <20% AI Detection | **Domain:** Metallurgy Papers

---

## ðŸš€ Quick Setup (10 Minutes)

### 1. Install Prerequisites

```bash
# Verify Python 3.10+
python --version

# Create project
mkdir ai-humanizer && cd ai-humanizer
```

### 2. Environment Setup

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install anthropic==0.40.0 deepl==1.19.1 spacy==3.8.2 \
            nltk==3.9.1 bert-score==0.3.13 PyPDF2==3.0.1 \
            pdfplumber==0.11.4 python-docx==1.1.2 python-dotenv==1.0.1

# Download models
python -m spacy download en_core_web_trf
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords')"
```

### 3. API Keys

Create `.env` file:

```bash
ANTHROPIC_API_KEY=sk-ant-api03-xxxxx
DEEPL_API_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

### 4. Directory Structure

```bash
mkdir -p data/glossary data/patterns projects
```

Create `data/glossary/metallurgy_terms.json`:

```json
{
  "terms": [
    "austenite", "martensite", "ferrite", "pearlite", "bainite",
    "quenching", "tempering", "annealing", "grain size", "hardness",
    "microstructure", "phase transformation", "heat treatment"
  ],
  "alloys": {
    "stainless_steel": ["304", "316L", "310", "duplex", "2507"]
  }
}
```

### 5. Run Example

```bash
# Place sample paper in projects/test/input/paper.md

# Run pipeline
python src/pipeline/pipeline.py \
  --input projects/test/input/paper.md \
  --output projects/test/output/humanized.md \
  --config config.yaml \
  --project projects/test

# Check output
cat projects/test/output/humanized.md
cat projects/test/quality_report.json
```

---

## ðŸ“Š Expected Results

| Metric | Target | Typical Result |
|--------|--------|----------------|
| **AI Detection** | <20% | 12-18% |
| **BERTScore** | â‰¥0.92 | 0.93-0.96 |
| **BLEU** | â‰¥0.80 | 0.82-0.90 |
| **Term Preservation** | 100% | 100% |
| **Processing Time** | 10-20 min | 12-18 min (5000 words) |
| **Cost** | $1-2 | $1.20 (typical) |

---

## ðŸ”§ Configuration (config.yaml)

```yaml
detection:
  target_threshold: 0.20    # <20% AI detection
  max_iterations: 5         # Per section

paraphrasing:
  primary_method: adversarial
  fallback_method: backtranslation

quality:
  min_bertscore: 0.92
  min_bleu: 0.80
```

---

## ðŸ› ï¸ Component Overview

### Processing Pipeline (Per Section)

```
Original Text
    â†“
1. Term Protection (1-2s)
   austenite â†’ <TERM_001>
    â†“
2. Adversarial Paraphrasing (10-30s)
   Claude Sonnet guided rewrite
    â†“
3. Fingerprint Removal (1-2s)
   Remove "Additionally", "Furthermore" clusters
    â†“
4. Burstiness Enhancement (2-5s)
   Vary sentence lengths (Methods: 15-25 words)
    â†“
5. Term Restoration (1s)
   <TERM_001> â†’ austenite
    â†“
6. Quality Validation (5-10s)
   BERTScore â‰¥ 0.92, BLEU â‰¥ 0.80
    â†“
7. Detection Analysis (5-15s)
   AI probability estimate
    â†“
IF detection > 20% â†’ ITERATE (max 5 times)
IF detection < 20% â†’ NEXT SECTION
```

---

## ðŸ“ Example Transformation

### Input (AI-generated, 99% detection):

> "The microstructural evolution was investigated using scanning electron microscopy. Additionally, the mechanical properties were evaluated through Vickers hardness testing. Furthermore, the phase transformations were analyzed via X-ray diffraction. It is important to note that the austenite grain size plays a crucial role in determining the material's performance."

### Output (Humanized, 16% detection):

> "Microstructural evolution was examined through scanning electron microscopy. Mechanical properties were assessed using Vickers hardness measurements. Phase transformations were characterized by X-ray diffraction. Austenite grain size significantly influences material performance."

**Changes:**
- âœ“ Removed "Additionally", "Furthermore" (AI fingerprints)
- âœ“ Removed "It is important to note", "plays a crucial role" (formulaic)
- âœ“ Varied sentence structure (active/passive mix)
- âœ“ Preserved technical terms (austenite, Vickers, X-ray diffraction)
- âœ“ Maintained quantitative accuracy
- âœ“ BERTScore: 0.94 (semantic similarity preserved)

---

## ðŸš¨ Troubleshooting

### Error: "ANTHROPIC_API_KEY not found"
**Solution:** Create `.env` file with API key

### Error: "spaCy model not found"
**Solution:** `python -m spacy download en_core_web_trf`

### Detection stays >20% after 5 iterations
**Solution:**
1. Check if technical content is too formulaic
2. Try hybrid paraphrasing method
3. Manual editing (see Human-in-Loop section)

### BERTScore <0.92 (quality degradation)
**Solution:** System auto-rollbacks to previous checkpoint. Check logs.

---

## ðŸ“ˆ Performance Optimization

### Speed Up Processing:
- Use smaller spaCy model: `en_core_web_sm` (faster, less accurate)
- Reduce max_iterations: 3 instead of 5
- Skip BLEU calculation (BERTScore only)

### Reduce API Costs:
- Use back-translation as primary method (no Claude usage)
- Lower temperature for Claude (0.5 â†’ less creative, faster)
- Cache results for identical sections

### Improve Detection Evasion:
- Increase max_iterations to 7
- Enable human-in-loop for borderline cases (15-20%)
- Use hybrid paraphrasing method

---

## ðŸ§ª Testing

### Unit Tests:
```bash
pytest tests/test_term_protector.py
pytest tests/test_paraphraser.py
pytest tests/test_validator.py
```

### Integration Test:
```bash
python src/pipeline/pipeline.py \
  --input data/examples/sample_paper.md \
  --output projects/test/output.md \
  --project projects/test
```

### Manual Component Test:
```python
from src.components.term_protector import TermProtector

protector = TermProtector("data/glossary/metallurgy_terms.json")
text = "AISI 304 steel at 1050Â°C"
protected = protector.protect(text)
print(protected)  # <TERM_001> steel at 1050<TERM_002>
```

---

## ðŸ“¦ Project Structure

```
ai-humanizer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/          # 8 processing components
â”‚   â”œâ”€â”€ pipeline/            # Main orchestration
â”‚   â””â”€â”€ utils/               # Helpers (state, file handling)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ glossary/            # Technical terms
â”‚   â””â”€â”€ patterns/            # AI fingerprints
â”œâ”€â”€ projects/                # Working directory
â”‚   â””â”€â”€ paper_001/
â”‚       â”œâ”€â”€ input/
â”‚       â”œâ”€â”€ checkpoints/     # Auto-saved states
â”‚       â””â”€â”€ output/
â”œâ”€â”€ tests/                   # Unit & integration tests
â”œâ”€â”€ .env                     # API keys (DO NOT COMMIT)
â”œâ”€â”€ config.yaml              # System configuration
â””â”€â”€ requirements.txt
```

---

## ðŸ“Š Cost Calculator

| Paper Length | Sections | Iterations | Claude Tokens | DeepL Chars | Total Cost |
|--------------|----------|------------|---------------|-------------|------------|
| 3000 words   | 5        | 2.5 avg    | ~25K          | 2K          | $0.50      |
| 5000 words   | 6        | 3 avg      | ~36K          | 5K          | $0.75      |
| 8000 words   | 7        | 3.5 avg    | ~50K          | 8K          | $1.20      |

**Formula:**
- Claude: (input_tokens Ã— $0.003/1K) + (output_tokens Ã— $0.015/1K)
- DeepL: characters Ã— $0.020/1K
- Total = Claude + DeepL

---

## ðŸŽ¯ Success Metrics

### Phase 1 (MVP - 2 weeks):
- âœ“ Basic pipeline working
- âœ“ 70-80% detection success rate
- âœ“ BERTScore â‰¥ 0.90

### Phase 2 (Production - 6 weeks):
- âœ“ All 8 components integrated
- âœ“ 90-95% detection success rate
- âœ“ BERTScore â‰¥ 0.92
- âœ“ Checkpointing & recovery
- âœ“ Human-in-loop for edge cases

### Phase 3 (Optimization - 8 weeks):
- âœ“ <10 min processing time (5000 words)
- âœ“ <$0.50 per paper
- âœ“ Batch processing (5+ papers concurrently)

---

## ðŸ“š Key Documents

1. **AI_HUMANIZER_TECHNICAL_ARCHITECTURE.md** - Complete technical design (120 pages)
2. **QUICK_START_GUIDE.md** - This document
3. **Phase 1 Report** - AI detection landscape
4. **Phase 2 Report** - Technical stack recommendations
5. **Phase 3 Report** - Metallurgy paper conventions

---

## ðŸ†˜ Support

### Common Issues:

**"Quality degradation after paraphrasing"**
â†’ System auto-rollbacks. Check `checkpoints/` for previous versions.

**"High detection after 5 iterations"**
â†’ Trigger manual editing or try hybrid method.

**"API rate limit exceeded"**
â†’ Add exponential backoff (built-in). Wait 60s and retry.

**"Technical terms corrupted"**
â†’ Term protector should prevent this. Check glossary completeness.

---

## ðŸš€ Next Steps

1. âœ… Complete environment setup (10 min)
2. âœ… Run example paper (5 min)
3. âœ… Review quality report (5 min)
4. ðŸ“ Process your first real paper
5. ðŸ”§ Tune configuration based on results
6. ðŸ“Š Track metrics over multiple papers
7. ðŸŽ¯ Aim for 95%+ success rate

---

**Ready to start? Run:**

```bash
python src/pipeline/pipeline.py \
  --input YOUR_PAPER.md \
  --output humanized.md \
  --project projects/my_paper
```

**Questions?** See `AI_HUMANIZER_TECHNICAL_ARCHITECTURE.md` for detailed explanations.

**Good luck!** ðŸŽ“
